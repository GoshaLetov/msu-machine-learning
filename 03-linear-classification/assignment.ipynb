{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd76aa8c",
   "metadata": {},
   "source": [
    "# 3.1. Бинарная кросс-энтропия\n",
    "\n",
    "В сегодняшней домашней работе вам предлагается самостоятельно написать и обучить модель логистической регрессии для решения задачи бинарной классификации.\n",
    "\n",
    "Реализация логистической регрессии с имеющимся заранее вектором весов \\vec{w} - задача достаточно простая. Вся загадка кроется в вопросе о том, как подобрать оптимальный вектор весов \\vec{w}. Прежде чем перейти к вопросу о подборе этого вектора необходимо определиться с тем, в каком смысле мы понимаем его оптимальность. То есть что является критерием его оптимальности? Как сказать, что один вектор \\vec{w_1} более оптимален, чем другой \\vec{w}?\n",
    "\n",
    "Для определения того, насколько хорош некоторый вектор параметров для решения нашей конкретной задачи, вводится понятие функции потерь (Loss-функции). Значение этой функции обратно пропорционально качеству решения нашей задачи. То есть оптимальным вектором параметров мы теперь можем назвать тот, при котором значение Loss-функции будет наименьшим из всех возможных. Таким образом мы сводим решение задачи машинного обучения к решению задачи оптимизации.\n",
    "\n",
    "Для задачи бинарной классификации очень популярна функция потерь, называемая бинарной кросс-энтропией. Эта функция пришла в машинное обучение из теории информации. Она имеет следующий вид:\n",
    "\n",
    "$$H(p, y) = - (y \\cdot ln(p) +(1 - y) \\cdot ln(1-p))$$\n",
    "\n",
    "\n",
    "Где $y$ - верный ответ (0 или 1), а $p$ - наше предположение, выраженное в оцененной степени принадлежности очередного объекта $x$ классу 1.\n",
    "\n",
    "Для обучения модели мы решим воспользоваться алгоритмом градиентного спуска.\n",
    "\n",
    "В связи с этим, нам необходимо предварительно посчитать все производные функции H по параметрам модели \\vec{w_1}...\\vec{w_n}.\n",
    "\n",
    "Выведите формулы этих производных и посчитайте производную функции H в точке\n",
    "\n",
    "$$w_0 = 0$$\n",
    "$$w_1 = 2$$\n",
    "$$w_2 = -1$$\n",
    "\n",
    "По каждому из этих параметров, если рассматриваемый объект x, имеющий координаты векторного представления \n",
    "$x_1=1$ и $x_2=$ принадлежит классу 1.\n",
    "\n",
    "В ответ в контесте внесите значение произведения $\\frac{∂H}{w_0}$$\\frac{∂H}{w_1}$$\\frac{∂H}{w_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c110d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = 1\n",
    "w = np.array([0, 2, -1])\n",
    "x = np.array([1, 1, 2])\n",
    "o = np.array([1, 1, 1])\n",
    "\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + np.math.exp(-x))\n",
    "\n",
    "\n",
    "p = sigmoid(np.dot(x, w))\n",
    "\n",
    "\n",
    "def f(x: float) -> float:\n",
    "    return - (1 - p) * x\n",
    "\n",
    "\n",
    "np.product([f(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3b873",
   "metadata": {},
   "source": [
    "# 3.2 Градиентный спуск\n",
    "\n",
    "В этом задании нам предстоит реализовать классический алгоритм градиентного спуска для обучения модели логистической регрессии.\n",
    "\n",
    "Алгоритм выполнения этого задания следующий:\n",
    "\n",
    "* На основе посчитанных в первом задании частных производных, напишем функцию подсчета градиента бинарной кросс-энтропии по параметрам модели\n",
    "\n",
    "* Напишем функцию обновления весов по посчитанным градиентам \n",
    "\n",
    "* Напишем функцию тренировки модели\n",
    "\n",
    "Замечание:\n",
    "Тренировка модели проводится в несколько циклов, в рамках каждого из которых мы обновим веса модели, основываясь на предсказании для **каждого** объекта из датасета. Такие циклы называются *эпохами*. То есть одна эпоха - это набор обновлений весов, реализованный согласно посчитанным для каждого объекта из датасета ошибкам модели.\n",
    "\n",
    "Вам необходимо реализовать обучение модели в несколько эпох. Их количество задается параметром функции. В рамках каждой эпохи необходимо пройти циклом по всем объектам обучающей выборки и обновить веса модели.\n",
    "\n",
    "# Замечание:\n",
    "\n",
    "В случае, если у Вас возникли сложности с выполнением первого задания и, как следствие, у Вас не выходит сделать это, мы рекомендуем подробно ознакомиться с главой **Производные $\\frac{\\partial H}{\\partial \\omega_i}$** нашей [лекции](https://colab.research.google.com/drive/1xjX_YnXcRr8HSiYLByMHxEIAADqs7QES?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe75341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Функция - Сигмоида\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Функция предсказания модели\n",
    "def predict(features: np.array, weights: np.array) -> float:\n",
    "    x = np.dot(features, weights)\n",
    "    return sigmoid(x)\n",
    "\n",
    "\n",
    "# Функция подсчета градиента\n",
    "def gradient(y_true: int, y_pred: float, x: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    y_true - истинное значение ответа для объекта x\n",
    "    y_pred - значение степени принадлежности объекта x классу 1, предсказанное нашей моделью\n",
    "    x - вектор признакового описания данного объекта\n",
    "\n",
    "    На выходе ожидается получить вектор частных производных H по параметрам модели, предсказавшей значение y_pred\n",
    "    Обратите внимание, что размерность этого градиента должна получиться на единицу больше размерности x засчет своободного коэффициента a0\n",
    "    \"\"\"\n",
    "    grad = x * (y_pred - y_true)\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Функция обновления весов\n",
    "def update(alpha: np.array, gradient: np.array, lr: float):\n",
    "    \"\"\"\n",
    "    alpha: текущее приближения вектора параметров модели\n",
    "    gradient: посчитанный градиент по параметрам модели\n",
    "    lr: learning rate, множитель перед градиентом в формуле обновления параметров\n",
    "    \"\"\"\n",
    "    alpha_new = alpha - lr * gradient\n",
    "    return alpha_new\n",
    "\n",
    "#функция тренировки модели\n",
    "def train(alpha0: np.array, x_train: np.array, y_train: np.array, lr: float, num_epoch: int):\n",
    "    \"\"\"\n",
    "    alpha0 - начальное приближение параметров модели\n",
    "    x_train - матрица объект-признак обучающей выборки\n",
    "    y_train - верные ответы для обучающей выборки\n",
    "    lr - learning rate, множитель перед градиентом в формуле обновления параметров\n",
    "    num_epoch - количество эпох обучения, то есть полных 'проходов' через весь датасет\n",
    "    \"\"\"\n",
    "    alpha = alpha0.copy()\n",
    "    for epo in range(num_epoch):\n",
    "        for i, x in enumerate(x_train):\n",
    "            x = np.hstack([x, 1])\n",
    "            prediction = predict(x, alpha)\n",
    "            grad = gradient(y_train[i], prediction, x)\n",
    "            alpha = update(alpha, grad, lr)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51480a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22b51059",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "\n",
    "alpha = np.zeros(n_features)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def gradient_boosting(X, y, lr, alpha=np.zeros(n_features), n_iter=100):\n",
    "    alpha = alpha.copy()\n",
    "    for i in range(n_iter):\n",
    "        grad = 2 * (X @ alpha - y) @ X\n",
    "        alpha -= lr * grad\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def stohastic_gradient_boosting(X, y, lr, alpha=np.zeros(n_features), n_iter=100):\n",
    "    alpha = alpha.copy()\n",
    "    for i in range(n_iter):\n",
    "        x = X[np.random.randint(0, 442, size=10)]\n",
    "        grad = 2 * (x @ alpha - y) @ X\n",
    "        alpha -= lr * grad\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "11c94819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([367,  34,   6, 303,  11,  53, 337, 165, 363, 236])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5298a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11.49892374, -15.30539586, -18.63225437,  -6.60545121,\n",
       "        13.35549652,  10.51587038,  13.10705539,   0.78286312,\n",
       "        -6.0123431 ,   5.3291298 ])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (X[0] @ alpha - y[0]) * X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c869116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -7.90862466, -237.46573876,  525.16977942,  322.33448712,\n",
       "       -335.67716673,  114.28444259, -102.85626314,  119.24542186,\n",
       "        581.06145778,   69.29580816])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boosting(X, y, 0.01, n_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "65043655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.52055262e-01, -2.23142385e+02,  4.78083673e+02,  3.14840524e+02,\n",
       "       -3.98494520e+01, -1.23654339e+02, -1.90395119e+02,  1.20275390e+02,\n",
       "        4.24323033e+02,  9.56434774e+01])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stohastic_gradient_boosting(X, y, 0.01, n_iter=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8541b2",
   "metadata": {},
   "source": [
    "# L2-регуляризация\n",
    "\n",
    "Регуляризация - это один из способов борьбы с переобучением моделей. Делается это в основном при помощи добавления к функции потерь некоторых слагаемых. Существуют стандартные методы регуляризации. Один из таких методов - это так называемая L2-регуляризация. Состоит идея L2-регуляризации в следующем:\n",
    "\n",
    "Мы добавляем к функции потерь специальное слагаемое, равное половине квадрата 2-нормы (длины вектора) весов модели, умноженного на некоторый коэффициент. То есть наша новая лосс функция выглядит так:\n",
    "\n",
    "$$\\hat{H} = H + \\frac{β}{2}|\\vec{\\omega}|^2$$\n",
    "\n",
    "Градиенты такой функции тоже нехитрым образом преобразуются:\n",
    "$$\\frac{∂ \\hat{H}}{∂ \\omega} = \\frac{∂H}{∂ \\omega} + β\\omega$$\n",
    "Поскольку $|\\vec{\\omega}|^2 = \\omega_0^2 + \\omega_1^2 + ... + \\omega_n^2$\n",
    "\n",
    "Реализуйте класс LogisticRegression для решения задачи бинарной классификации с L2-регуляризацией.\n",
    "\n",
    "Напоминание:\n",
    "\n",
    "* Функция .fit(x, y) производит обучение модели. В рамках этой функции необходимо реализовать подбор оптимальных параметров модели/сконфигурировать модель для дальнейшего использования на основе данной тренировочной выборки, где x - это матрица признакового описания выборки, а y - вектор ответов\n",
    "\n",
    "* Функция .predict(x) осуществляет предсказание для каждого из объектов, чьи векторные описания представлены строками матрицы x. Выполняется строго после .fit(). Ради безопасности можно даже реализовать механизм отказа в виде выбрасывания специальной ошибки UnfittedError в случае попытки вызова функции .predict() до вызова функции .fit()\n",
    "\n",
    "# Замечание\n",
    "\n",
    "По большому счету Вам нужно внести соответствующие изменения в класс LogisticRegression, который Вы реализовали в предыдущем задании. В качестве шаблона кода можно также взять код из предыдущего задания. Главное требование - реализация функций `.fit()`,  `.predict()` и `predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7a83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class UnfittedError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alpha = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x: float) -> float:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_constant_column(X: np.array) -> np.array:\n",
    "        if len(X.shape) > 1:\n",
    "            return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "        return np.hstack([X, 1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gradient(y_true: int, y_pred: float, x: np.array) -> np.array:\n",
    "        grad = x * (y_pred - y_true)\n",
    "        return grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _update(alpha: np.array, gradient: np.array, lr: float) -> np.array:\n",
    "        alpha_new = alpha - lr * gradient\n",
    "        return alpha_new\n",
    "    \n",
    "    def fit(self, x_train: np.array, y_train: np.array, lr: float, betta: float, num_epoch: int) -> None:\n",
    "        alpha = np.ones(x_train.shape[1] + 1)\n",
    "        for epo in range(num_epoch):\n",
    "            for i, x in enumerate(x_train):\n",
    "                x = self._add_constant_column(x)\n",
    "                prediction = self._sigmoid(np.dot(x, alpha))\n",
    "                grad = self._gradient(y_train[i], prediction, x) + betta * alpha\n",
    "                alpha = self._update(alpha, grad, lr)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.alpha is None:\n",
    "            raise UnfittedError\n",
    "        preds = self._sigmoid(np.dot(self._add_constant_column(X), self.alpha))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3fcadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
